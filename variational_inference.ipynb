{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this tutorial is to implement a Variational Autoencoder (VAE). The aim is to give you sense of:\n",
    "\n",
    "1. How the \"*reparametrization trick*\" enables scaling of variational inference to large scale datasets\n",
    "2. How we can amortize the inference by learning a *recognition model*\n",
    "\n",
    "The combination of a generative model from low dimensional latent variables $z$ to high dimensional observations $x$, combined with a recognition model from $x$ to $z$ is what gives rise to the autoencoding structure. The approach we follow here was first proposed by Kingma et al. (https://arxiv.org/abs/1312.6114) and Rezende et. al. (https://arxiv.org/pdf/1401.4082.pdf).\n",
    "\n",
    "As a brief refresher, variational inference is a general approach to doing approximate inference for intractable posterior distributions. For example, we might have a probabilistic model with observations $x$ and latent variables $z$, and we are interested in computing the posterior distribution $p(z|x)$. For relatively complex data, we might imagine the generative process from $z \\to x$ to be some very complicated function we want to learn using a neural network $x = f_{\\theta}(z)$. In order to compute the posterior distribution we need the normalizing factor $p(x) = \\int p(x|f_{\\theta}(z))p(z)dz$ which is intractable due to the non-linear mapping we have imposed with the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting model and train (hyper)parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 20\n",
    "input_dim = 784\n",
    "learning_rate = 5e-3\n",
    "log_interval = 50\n",
    "num_workers = 8\n",
    "z_dim = 2\n",
    "\n",
    "# cuda\n",
    "cuda = False\n",
    "device = torch.device(\"cuda\" if cuda and torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and visualize MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST database is a database of handwritten digits that is commonly used for training and testing in the field of machine learning and computer vision. The MNIST database contains 60,000 training images and 10,000 testing images. Each image is of size $28 \\times 28$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_dl(batch_size, data_dir='/tmp/data'):\n",
    "    transf = transforms.ToTensor()\n",
    "    mnist_train = datasets.MNIST(data_dir, train=True, download=True, transform=transf)\n",
    "    mnist_test = datasets.MNIST(data_dir, train=False, transform=transf)\n",
    "    dl_kwargs = {\n",
    "        'batch_size': batch_size,\n",
    "        'shuffle': True,\n",
    "        'num_workers': num_workers,\n",
    "        'drop_last': True\n",
    "    }\n",
    "    train_loader = DataLoader(mnist_train, **dl_kwargs)\n",
    "    test_loader = DataLoader(mnist_test, **dl_kwargs)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get a data loader for each MNIST train and test set\n",
    "train_loader, test_loader = load_mnist_dl(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "# get the next batch from the train data loader\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images from dataset\n",
    "plt.figure(figsize=(5, 5))\n",
    "img_grid = make_grid(images, nrow=10)\n",
    "_ = plt.imshow(img_grid.numpy().transpose(1, 2, 0))\n",
    "_ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Process\n",
    "\n",
    "Let us consider a dataset $X = \\{x_i\\}^N_{i=1}$ consisting of N samples of some continuous or discrete variable x. \n",
    "We assume that the data are generated by some random process.\n",
    "It starts with a latent representation $z$ sampled from a simple distribution, e.g. a standard Gaussian distribution with diagonal covariance. \n",
    "Then the samples are fed through a deep neural network ($f_{\\theta}$) to capture the complex generative process of high dimensional observations such as images.\n",
    "For binarized MNIST, the observation is chosen to be Bernoulli, with its parameters also parametrized by a neural network.\n",
    "\\begin{align}\n",
    "z &\\sim \\mathcal{N}(0, \\text{I})\\\\\n",
    "x_{\\text{logits}} &= f_{\\theta}(z)\\\\\n",
    "x &\\sim \\text{Bernoulli}(x; S(x_{\\text{logits}})), \n",
    "\\end{align}\n",
    "\n",
    "where $S(x) = \\frac{1}{1 + e^{-x}}$ is the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "The objective is to approximate the posterior distribution $p_{\\theta}(z|x)$. To do so, we use a mean field variational approximation. In the setting of the variational autoencoder, the variational posterior $q_{\\phi}(z|x)$ is also parameterized by a neural network $f_{\\phi}$ which takes input $x$ and outputs the mean and variance of a Normal distribution:\n",
    "\n",
    "\\begin{align}\n",
    "\\mu(x;\\phi), \\log \\sigma^2(x;\\phi) &=f_{\\phi}(x)\\\\\n",
    "q_{\\phi}(z|x) &= \\mathcal{N}(z|\\mu(x;\\phi),\\sigma^2(x;\\phi))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because directly optimizing $\\log p_{\\theta}(x)$ is infeasible, we choose to optimize a lower bound $\\mathcal{L}$ of it. The lower bound on the marginal likelihood of datapoint $i$ can be written as:\n",
    "\\begin{align}\n",
    "\\log p_{\\theta}(x_i) \\ge \\mathcal{L}(x_i; \\theta, \\phi) &= \\mathbb{E}_{q_{\\phi}(z_i|x_i)}[\\log p_{\\theta}(x_i,z_i) - \\log q_{\\phi}(z_i|x_i)]\\\\\n",
    "&= \\mathbb{E}_{q_{\\phi}(z_i|x_i)}[\\log p_{\\theta}(x|z)] - D_{KL}(q_{\\phi}(z_i|x_i) \\parallel p(z_i))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KL term can be integrated analytically in our case, the expected reconstruction error $\\mathbb{E}_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)]$ requires estimation by sampling:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{q_{\\phi}(z_i|x_i)}[\\log p_{\\theta}(x_i|z_i)] \\simeq \\frac{1}{L} \\sum_{l=1}^L \\log p_{\\theta}(x_i|z_i^l), \\; z_i^l \\sim q_{\\phi}(z_i | x_i)\n",
    "\\end{align}\n",
    "\n",
    "According to the original paper, L can be set to 1 if the batch size is large enough ($\\ge 100$).\n",
    "\n",
    "The ELBO over one batch can be calculated with:\n",
    "\\begin{align}\n",
    "\\mathcal{L}(X; \\theta, \\phi) = \\frac{1}{M} \\sum_{i=1}^M \\big(\\sum_{l=1}^L \\log p_{\\theta}(x_i|z_i^l) - D_{KL}(q_{\\phi}(z_i|x_i) \\parallel p(z_i))\\big)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation\n",
    "Pytorch uses automatic differentiation (autograd) to automate the computation of backward passes in neural networks. When using autograd, the *forward* pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: The reparametrization trick\n",
    "\n",
    "One of the key aspects of the Kingma et. al. (https://arxiv.org/abs/1312.6114) paper was the introduction of the reparametrization trick. The problem we run into with training the VAE is that we need the gradient of an expectation of the lower bound w.r.t. the parameters of the variational distribution. I.e. $\\nabla_{\\phi}\\mathcal{L}(x_i; \\theta, \\phi)$ in the equations above. Without going into detail (see http://blog.shakirm.com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks/ for a great tutorial), we can avoid this problem for some distributions by reparameterizing our random variable in terms of a deterministic function and a random variable that is independent of the parameters we wish to take the gradient with respect to.\n",
    "\n",
    "For a Gaussian distribution, we can get an unbias estimate of the monte-carlo approximation of the expectations by taking a sample from a normal gaussian $\\epsilon \\sim \\mathcal{N}(0, I)$ and the reparameterized sample is then given by\n",
    "\n",
    "\\begin{align}\n",
    "z_i^l = \\mu(x;\\phi) + \\sigma(x;\\phi) * \\epsilon\n",
    "\\end{align}\n",
    "\n",
    "Finish the *reparametrize* function in the VAE class below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim=784, z_dim=2):\n",
    "        super(VAE, self).__init__()\n",
    "        # Use the nn.Module package to define the VAE model as a sequence of layers.\n",
    "        # nn.Sequential s a Module which contains other Modules, \n",
    "        # and applies them in sequence to produce its output.\n",
    "        # Each Linear module (nn.Linear) computes output from input using a\n",
    "        # linear function, and holds internal Tensors for its weight and bias.\n",
    "\n",
    "        # inference network q(z|x)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # mean and log(var)\n",
    "        self.mu = nn.Linear(128, z_dim)\n",
    "        self.logvar = nn.Linear(128, z_dim)\n",
    "        \n",
    "        # generator network p(x|z)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Use mu and log(var) to sample z ~ N(mu, var)\n",
    "        Parameters\n",
    "        ----------\n",
    "        mu: pytorch Tensor of shape (N, z_dim)\n",
    "        logvar: pytorch Tensor of shape (N, z_dim)\n",
    "        Returns\n",
    "        -------\n",
    "        z: pytorch Tensor of shape (N, z_dim)\n",
    "        \"\"\"\n",
    "        e = torch.randn_like(mu)\n",
    "        # Edit this function        \n",
    "        z = None\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        -----------\n",
    "        x: pytorch Tensor of shape (N, input_dim)\n",
    "        Returns\n",
    "        -------\n",
    "        recon_x: pytorch Tensor of shape like x\n",
    "        mu: pytorch Tensor of shape (N, z_dim)\n",
    "        logvar: pytorch Tensor of shape (N, z_dim)\n",
    "        \"\"\"\n",
    "        h = self.encoder(x)\n",
    "        mu = self.mu(h)\n",
    "        logvar = self.logvar(h)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        recon_x = self.decoder(z)\n",
    "        return recon_x, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: KL divergence\n",
    "In the function KL you are supposed return the KL between a parametrized Gaussian distribution $\\mathcal{N}(\\mu(x, \\phi), \\sigma^2(x, \\phi))$ and $\\mathcal{N}(0, I)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(mu, logvar):\n",
    "    \"\"\"\n",
    "    Computes the KL divergence between N(mu, exp(logvar)I) and N(0, I).\n",
    "    Parameters\n",
    "    ----------\n",
    "    mu: pytorch Tensor of shape (N, D)\n",
    "    logvar: pytorch Tensor of shape (N, D)\n",
    "    Returns: \n",
    "    --------\n",
    "    kl: pytorch Tensor of shape (N,)\n",
    "    \"\"\"\n",
    "    # Edit this function\n",
    "    kl = None\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Objective function\n",
    "By changing our objective from maximizing the ELBO to the equivalent objective of minimizing the negative ELBO, we can use gradient descent methods to optimize variational parameters. *loss_func* implements the negative ELBO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    mu: pytorch Tensor of shape (N, z_dim)\n",
    "    logvar: pytorch Tensor of shape (N, z_dim)\n",
    "    recon_x: pytorch Tensor of shape (N, input_dim)\n",
    "             reconstruction of x\n",
    "    x: pytorch Tensor of shape (N, input_dim)\n",
    "    Returns:\n",
    "    L: \n",
    "    negative ELBO \n",
    "    -------\n",
    "    \"\"\"\n",
    "    # Edit this function\n",
    "    L = None  \n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(input_dim=input_dim, z_dim=z_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this should not take more than 5 min to train for 20 epochs\n",
    "\n",
    "N_dl_tr = len(train_loader)\n",
    "N_data_tr = len(train_loader.dataset)\n",
    "N_data_te = len(test_loader.dataset)\n",
    "N_dl_te = len(test_loader)\n",
    "desc_str_tr = 'Epoch {} (Train)'\n",
    "desc_str_te = 'Epoch {} (Test)'\n",
    "\n",
    "loss_train_hist = []\n",
    "loss_test_hist = []\n",
    "ebar = tqdm_notebook(range(epochs), desc='[Epoch {}]'.format(1))\n",
    "for epoch in ebar:\n",
    "    nbar = tqdm_notebook(enumerate(train_loader),\n",
    "                         total=N_dl_tr,\n",
    "                         desc='Training...',\n",
    "                         leave=False)\n",
    "    loss_train = 0.\n",
    "    model.train()\n",
    "    for i, (data, labels) in nbar:\n",
    "        # dynamically binarize data\n",
    "        n = data.shape[0]\n",
    "        data = torch.bernoulli(data).to(device)\n",
    "        data = data.view(data.shape[0], -1)\n",
    "\n",
    "        # calculate loss \n",
    "        recon_x, mu, logvar = model(data)\n",
    "        loss = loss_func(recon_x, data, mu, logvar)\n",
    "        \n",
    "        # Zero the gradients before running the backward pass.\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "        # parameters of the model. Internally, the parameters of each Module are stored\n",
    "        # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "        # all learnable parameters in the model.\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights using gradient descent.\n",
    "        # Each parameter is a Tensor, so\n",
    "        # we can access its gradients like we did before.\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "\n",
    "        loss_train += loss.detach().item() * n / N_data_tr\n",
    "    loss_train_hist.append(loss_train)\n",
    "    \n",
    "    # evaluate on test dataset\n",
    "    model.eval()\n",
    "    loss_test = 0.\n",
    "    with torch.no_grad():\n",
    "        for i, (data, labels) in enumerate(test_loader):\n",
    "            n = data.shape[0]\n",
    "            data = torch.bernoulli(data).to(device)\n",
    "            data = data.view(data.shape[0], -1)\n",
    "            recon_x, mu, logvar = model(data)\n",
    "            loss_test += loss_func(recon_x, data, mu, logvar).item() * n / N_data_te\n",
    "    loss_test_hist.append(loss_test)\n",
    "    ebar.set_description('[Epoch {}/{}] train: {:.4f} test: {:.4f}'.format(\n",
    "        epoch + 1, epochs, loss_train, loss_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ELBO, samples, reconstruction and latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELBO history\n",
    "plt.figure()\n",
    "_ = plt.plot(range(epochs), np.asarray(loss_train_hist) * -1, label='Train')\n",
    "_ = plt.plot(range(epochs), np.asarray(loss_test_hist) * -1, label='Test')\n",
    "_ = plt.ylabel('ELBO')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "z_test = torch.Tensor(N_data_te, z_dim)\n",
    "labels_test = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (data, labels) in enumerate(test_loader):\n",
    "        data = torch.bernoulli(data).to(device)\n",
    "        data = data.view(data.shape[0], -1)\n",
    "        recon_x, mu, logvar = model(data)\n",
    "        z_test[i * batch_size:i * batch_size + data.shape[0], :] = model.reparametrize(mu, logvar)\n",
    "        labels_test.append(labels)\n",
    "\n",
    "labels_test = torch.cat(labels_test).numpy()\n",
    "z_test = z_test.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot reconstruction\n",
    "n = min(recon_x.shape[0], 16)\n",
    "recon_data = torch.cat([data[:n].view(n, 1, 28, 28),\n",
    "                        recon_x[:n].view(n, 1, 28, 28)], 0)\n",
    "grid_data = make_grid(recon_data, nrow=n)\n",
    "plt.figure(figsize=(40, 40))\n",
    "_ = plt.imshow(grid_data.numpy().transpose(1, 2, 0), cmap='gray')\n",
    "_ = plt.xticks([])\n",
    "_ = plt.yticks([14, 42], ['Data', 'Reconstruction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from model\n",
    "z = torch.randn(64, z_dim).to(device)\n",
    "with torch.no_grad():\n",
    "    recon_x = model.decoder(z)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "_ = plt.imshow(make_grid(recon_x.view(64, 1, 28, 28)).cpu().numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate through latent if z_dim = 2\n",
    "if z_dim == 2:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    x = y = torch.linspace(-4, 4, steps=20)\n",
    "    xv, yv = torch.meshgrid((x, y))\n",
    "    z = torch.cat((xv.flatten().unsqueeze(1), yv.flatten().unsqueeze(1)), 1)\n",
    "    with torch.no_grad():\n",
    "        recon_x = model.decoder(z)\n",
    "    _ = plt.imshow(make_grid(recon_x.view(-1, 1, 28, 28), nrow=20).cpu().numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot latent space if z_dim = 2\n",
    "if z_dim == 2:\n",
    "    for y in range(10):\n",
    "        z_test_y = z_test[labels_test == y]\n",
    "        plt.scatter(z_test_y[:, 0], z_test_y[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Different things you might want to try:\n",
    "* Try a different *z_dim*. You can use t-SNE to visualize your latent space if *z_dim* > 2.\n",
    "* Try a different observation distribution, e.g., Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further reading\n",
    "\n",
    "Generative models such as the VAE have been the focus of a flurry of research / papers for a while now. While the VAE is an extremely interesting [model + inference] approach, you may have heard of others such as generative adversarial networks (GANs) or autoregressive models such as the PixelRNN. All of these approaches have their pros and cons as it stands and are worth reading about in their own right if you are interested in generative models.\n",
    "\n",
    "* **Generative Adversarial Networks**, Ian Goodfellow et. al.: https://arxiv.org/abs/1406.2661\n",
    "* **Pixel Recurrent Neural Networks**, Aaron van den Oord et. al.: https://arxiv.org/abs/1601.06759\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
